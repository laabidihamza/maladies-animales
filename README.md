# ğŸ¾ Projet Web Scraping - Maladies Animales

SystÃ¨me automatisÃ© d'extraction et d'analyse de news sur les maladies animales avec Selenium et LLM.

## ğŸ“‹ Description

Ce projet extrait automatiquement des informations Ã  partir d'URLs de news sur les maladies animales et gÃ©nÃ¨re un dataset CSV structurÃ© avec :
- MÃ©tadonnÃ©es (titre, langue, dates, lieux)
- Analyse de contenu (maladie, animal concernÃ©)
- RÃ©sumÃ©s automatiques (50, 100, 150 mots)

## ğŸ—ï¸ Architecture

```
animal_disease_scraper/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/           # Fichier URLs d'entrÃ©e
â”‚   â”œâ”€â”€ output/          # RÃ©sultats (CSV)
â”‚   â””â”€â”€ logs/            # Logs d'exÃ©cution
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py       # Module Selenium
â”‚   â”œâ”€â”€ llm_analyzer.py  # Module LLM
â”‚   â”œâ”€â”€ utils.py         # Utilitaires
â”‚   â””â”€â”€ config.py        # Configuration
â””â”€â”€ main.py              # Script principal
```

## ğŸš€ Installation

### 1. PrÃ©requis
```bash
# Python 3.8+
python --version

# Chrome/Chromium installÃ© sur votre systÃ¨me
```

### 2. Installation des dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3. Installation d'Ollama (LLM local gratuit)

**Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2
```

**Windows:**
- TÃ©lÃ©charger depuis https://ollama.com/download
- Installer et exÃ©cuter
- Ouvrir terminal: `ollama pull llama3.2`

**VÃ©rifier l'installation:**
```bash
ollama list
```

## ğŸ“Š PrÃ©paration des DonnÃ©es

### Format du fichier d'entrÃ©e

CrÃ©ez `data/input/urls.csv` avec le format suivant :

```csv
code,lien
1,https://example.com/news1
2,https://example.com/news2
...
```

**Colonnes obligatoires:**
- `code` : Identifiant unique (numÃ©rique ou alphanumÃ©rique)
- `lien` : URL complÃ¨te de la news

## ğŸ¯ Utilisation

### Option 1 : ExÃ©cution complÃ¨te (recommandÃ©e)
bashpython main.py
```

## ğŸ“Š Flux d'ExÃ©cution
```
main.py
   â”‚
   â”œâ”€â†’ VÃ©rifier fichier d'entrÃ©e (urls.csv)
   â”‚
   â”œâ”€â†’ Phase 1: Scraping
   â”‚      â”‚
   â”‚      â”œâ”€â†’ NewsScaper.scrape_all_urls()
   â”‚      â””â”€â†’ Sauvegarde: scraped_data.csv
   â”‚
   â””â”€â†’ Phase 2: Analyse LLM
          â”‚
          â”œâ”€â†’ LLMAnalyzer.process_all()
          â””â”€â†’ Sauvegarde: final_dataset.csv
```

### Option 2 : ExÃ©cution par phase

**Phase 1 : Scraping uniquement**
```bash
python main.py --phase scraping
```

**Phase 2 : Analyse LLM uniquement**
```bash
python main.py --phase analysis
```

## ğŸ“ Fichiers de Sortie

### 1. `data/output/scraped_data.csv`
RÃ©sultats du scraping (Phase 1) :
- code, url, titre, contenu, langue
- nb_caracteres, nb_mots

### 2. `data/output/final_dataset.csv`
Dataset final complet avec toutes les colonnes :
- Informations de base + analyse LLM
- Date, lieu, maladie, source
- RÃ©sumÃ©s (50/100/150 mots)
- Animal concernÃ©

## âš™ï¸ Configuration

### Modifier le LLM utilisÃ©

Ã‰ditez `src/config.py` :

```python
LLM_CONFIG = {
    "provider": "ollama",  # ou "openai"
    "model": "llama3.2",   # ou "gpt-3.5-turbo"
    # ...
}
```

### Ajuster les paramÃ¨tres Selenium

```python
SELENIUM_CONFIG = {
    "headless": True,      # False pour voir le navigateur
    "timeout": 30,         # Timeout en secondes
    # ...
}
```

### Grok / xAI API key (cloud provider)

This project supports using Grok (xAI) in the cloud. The code prefers the variable name `GROQ_API_KEY` (exposed by `src/config.py`) but will fall back to the legacy `GROK_API_KEY` environment variable if present. Set one of these in a `.env` file at the project root or in `src/.env`.

Example `.env` content (never commit real secrets):

```dotenv
# Preferred name used by the code/config
GROQ_API_KEY=sk-<your_real_secret_here>

# Legacy name accepted by the code
GROK_API_KEY=sk-<your_real_secret_here>

# Optional OpenAI key
OPENAI_API_KEY=sk-<your_openai_secret>
```

Notes:
- Make sure you paste the actual secret token (starts with `sk-...`) and do NOT include surrounding quotes.
- If you receive an "Incorrect API key" error from the API, confirm the key is active/valid at https://console.x.ai and was copied fully.

## ğŸ”§ RÃ©solution de ProblÃ¨mes

### Erreur : "ChromeDriver not found"
```bash
# Le script tÃ©lÃ©charge automatiquement ChromeDriver
# Si problÃ¨me persiste :
pip install --upgrade webdriver-manager
```

### Erreur : "Ollama connection refused"
```bash
# VÃ©rifier qu'Ollama est dÃ©marrÃ©
ollama serve

# Dans un autre terminal
ollama list
```

### Contenu non extrait correctement
- VÃ©rifier que le site n'est pas protÃ©gÃ© par Cloudflare/Captcha
- Augmenter le dÃ©lai dans `scraper.py` (ligne `time.sleep(2)`)
- DÃ©sactiver le mode headless pour dÃ©bugger

### Langue non dÃ©tectÃ©e
- Le texte doit contenir au moins 10 caractÃ¨res
- Pour l'arabe, vÃ©rifier l'encodage UTF-8

## ğŸ¨ Personnalisation

### Ajouter de nouveaux champs

1. Modifier `config.py` :
```python
OUTPUT_COLUMNS = [
    ...,
    "nouveau_champ"
]
```

2. Modifier le prompt dans `llm_analyzer.py`

3. Ajuster `_validate_results()`

### Changer le format de date

Modifier la regex dans `utils.py` :
```python
pattern = r'^\d{2}-\d{2}-\d{4}$'  # jj-mm-aaaa
```

## ğŸ“Š Exemples de RÃ©sultats

### Exemple de ligne dans le dataset final :

| code | url | titre | langue | maladie | animal | lieu |
|------|-----|-------|--------|---------|--------|------|
| 1 | https://... | Alerte sanitaire | franÃ§ais | fiÃ¨vre aphteuse | bovins | Normandie |

## ğŸ¤ Alternatives LLM

### Ollama (RecommandÃ©)
âœ… Gratuit, local, pas de limite
âœ… Multilingue excellent
âœ… Pas besoin d'API key

### OpenAI
- CrÃ©dit gratuit initial ($5)
- Modifier `LLM_CONFIG` dans config.py
- Ajouter API key dans `.env`

### Groq (Alternative cloud gratuite)
- API gratuite avec limite journaliÃ¨re
- TrÃ¨s rapide
- Modifier config et ajouter `GROQ_API_KEY`

## ğŸ“ˆ Performance

- **Scraping** : ~2-5 secondes par URL
- **Analyse LLM** : ~10-20 secondes par article (Ollama local)
- **50 URLs** : ~15-30 minutes total

## ğŸ› Logs

Tous les logs sont dans `data/logs/scraping.log`

```bash
# Voir les logs en temps rÃ©el
tail -f data/logs/scraping.log
```

## ğŸ“š DÃ©pendances Principales

- **Selenium** : Automatisation du navigateur
- **BeautifulSoup4** : Parsing HTML
- **pandas** : Manipulation de donnÃ©es
- **langdetect** : DÃ©tection de langue
- **ollama** : Interface LLM local

## ğŸ’¡ Conseils

1. **Testez d'abord sur 5-10 URLs** avant de lancer sur 50
2. **VÃ©rifiez la qualitÃ©** du scraping avant l'analyse LLM
3. **Ajustez les timeouts** si les sites sont lents
4. **Utilisez des proxies** si vous scrapez beaucoup d'URLs

## ğŸ“ Support

Pour toute question :
1. VÃ©rifier les logs dans `data/logs/`
2. Tester avec `--phase scraping` d'abord
3. VÃ©rifier qu'Ollama fonctionne : `ollama list`

## ğŸ“„ Licence

Projet acadÃ©mique - Utilisez de maniÃ¨re responsable et Ã©thique.

---

**Bon scraping ! ğŸš€**